<!DOCTYPE html>
<html class="writer-html5" lang="en">
<head>
  <meta charset="utf-8" /><meta name="generator" content="Docutils 0.19: https://docutils.sourceforge.io/" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>sgGWR.optimizers package &mdash; sgGWR 0.1.0 documentation</title>
      <link rel="stylesheet" type="text/css" href="_static/pygments.css" />
      <link rel="stylesheet" type="text/css" href="_static/css/theme.css" />

  
  <!--[if lt IE 9]>
    <script src="_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
        <script src="_static/jquery.js"></script>
        <script src="_static/_sphinx_javascript_frameworks_compat.js"></script>
        <script data-url_root="./" id="documentation_options" src="_static/documentation_options.js"></script>
        <script src="_static/doctools.js"></script>
        <script src="_static/sphinx_highlight.js"></script>
        <script crossorigin="anonymous" integrity="sha256-Ae2Vz/4ePdIu6ZyI/5ZGsYnb+m0JlOmKPjt6XZ9JJkA=" src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
    <script src="_static/js/theme.js"></script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="prev" title="sgGWR package" href="sgGWR.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >

          
          
          <a href="index.html" class="icon icon-home">
            sgGWR
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">Tutorials:</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="examples/introduction.html">Let’s start GWR bandwidth calibration with <code class="docutils literal notranslate"><span class="pre">sgGWR</span></code>!</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Package References:</span></p>
<ul class="current">
<li class="toctree-l1 current"><a class="reference internal" href="sgGWR.html">sgGWR package</a><ul class="current">
<li class="toctree-l2 current"><a class="reference internal" href="sgGWR.html#subpackages">Subpackages</a><ul class="current">
<li class="toctree-l3 current"><a class="current reference internal" href="#">sgGWR.optimizers package</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#submodules">Submodules</a></li>
<li class="toctree-l4"><a class="reference internal" href="#module-sgGWR.optimizers.existings">sgGWR.optimizers.existings module</a></li>
<li class="toctree-l4"><a class="reference internal" href="#module-sgGWR.optimizers.existings_numpy">sgGWR.optimizers.existings_numpy module</a></li>
<li class="toctree-l4"><a class="reference internal" href="#module-sgGWR.optimizers.second">sgGWR.optimizers.second module</a></li>
<li class="toctree-l4"><a class="reference internal" href="#module-sgGWR.optimizers.sg">sgGWR.optimizers.sg module</a></li>
<li class="toctree-l4"><a class="reference internal" href="#module-sgGWR.optimizers.sg_numpy">sgGWR.optimizers.sg_numpy module</a></li>
<li class="toctree-l4"><a class="reference internal" href="#module-sgGWR.optimizers.vr">sgGWR.optimizers.vr module</a></li>
<li class="toctree-l4"><a class="reference internal" href="#module-sgGWR.optimizers">Module contents</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="sgGWR.html#submodules">Submodules</a></li>
<li class="toctree-l2"><a class="reference internal" href="sgGWR.html#module-sgGWR.kernels">sgGWR.kernels module</a></li>
<li class="toctree-l2"><a class="reference internal" href="sgGWR.html#module-sgGWR.models">sgGWR.models module</a></li>
<li class="toctree-l2"><a class="reference internal" href="sgGWR.html#module-sgGWR">Module contents</a></li>
</ul>
</li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">sgGWR.optimizers package</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#submodules">Submodules</a></li>
<li class="toctree-l2"><a class="reference internal" href="#module-sgGWR.optimizers.existings">sgGWR.optimizers.existings module</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#sgGWR.optimizers.existings.optax_optimizer"><code class="docutils literal notranslate"><span class="pre">optax_optimizer</span></code></a><ul>
<li class="toctree-l4"><a class="reference internal" href="#sgGWR.optimizers.existings.optax_optimizer.run"><code class="docutils literal notranslate"><span class="pre">optax_optimizer.run()</span></code></a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#sgGWR.optimizers.existings.scipy_L_BFGS_B"><code class="docutils literal notranslate"><span class="pre">scipy_L_BFGS_B</span></code></a><ul>
<li class="toctree-l4"><a class="reference internal" href="#sgGWR.optimizers.existings.scipy_L_BFGS_B.run"><code class="docutils literal notranslate"><span class="pre">scipy_L_BFGS_B.run()</span></code></a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#module-sgGWR.optimizers.existings_numpy">sgGWR.optimizers.existings_numpy module</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#sgGWR.optimizers.existings_numpy.scipy_L_BFGS_B"><code class="docutils literal notranslate"><span class="pre">scipy_L_BFGS_B</span></code></a><ul>
<li class="toctree-l4"><a class="reference internal" href="#sgGWR.optimizers.existings_numpy.scipy_L_BFGS_B.run"><code class="docutils literal notranslate"><span class="pre">scipy_L_BFGS_B.run()</span></code></a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#module-sgGWR.optimizers.second">sgGWR.optimizers.second module</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#sgGWR.optimizers.second.SGN"><code class="docutils literal notranslate"><span class="pre">SGN</span></code></a><ul>
<li class="toctree-l4"><a class="reference internal" href="#sgGWR.optimizers.second.SGN.run"><code class="docutils literal notranslate"><span class="pre">SGN.run()</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="#sgGWR.optimizers.second.SGN.step"><code class="docutils literal notranslate"><span class="pre">SGN.step()</span></code></a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#sgGWR.optimizers.second.SGN_BFGS"><code class="docutils literal notranslate"><span class="pre">SGN_BFGS</span></code></a><ul>
<li class="toctree-l4"><a class="reference internal" href="#sgGWR.optimizers.second.SGN_BFGS.step"><code class="docutils literal notranslate"><span class="pre">SGN_BFGS.step()</span></code></a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#sgGWR.optimizers.second.SGN_LM"><code class="docutils literal notranslate"><span class="pre">SGN_LM</span></code></a><ul>
<li class="toctree-l4"><a class="reference internal" href="#sgGWR.optimizers.second.SGN_LM.step"><code class="docutils literal notranslate"><span class="pre">SGN_LM.step()</span></code></a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#module-sgGWR.optimizers.sg">sgGWR.optimizers.sg module</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#sgGWR.optimizers.sg.ASGD"><code class="docutils literal notranslate"><span class="pre">ASGD</span></code></a><ul>
<li class="toctree-l4"><a class="reference internal" href="#sgGWR.optimizers.sg.ASGD.lr_schedule"><code class="docutils literal notranslate"><span class="pre">ASGD.lr_schedule()</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="#sgGWR.optimizers.sg.ASGD.step"><code class="docutils literal notranslate"><span class="pre">ASGD.step()</span></code></a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#sgGWR.optimizers.sg.Adam"><code class="docutils literal notranslate"><span class="pre">Adam</span></code></a><ul>
<li class="toctree-l4"><a class="reference internal" href="#sgGWR.optimizers.sg.Adam.step"><code class="docutils literal notranslate"><span class="pre">Adam.step()</span></code></a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#sgGWR.optimizers.sg.SGD"><code class="docutils literal notranslate"><span class="pre">SGD</span></code></a><ul>
<li class="toctree-l4"><a class="reference internal" href="#sgGWR.optimizers.sg.SGD.lr_schedule"><code class="docutils literal notranslate"><span class="pre">SGD.lr_schedule()</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="#sgGWR.optimizers.sg.SGD.run"><code class="docutils literal notranslate"><span class="pre">SGD.run()</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="#sgGWR.optimizers.sg.SGD.step"><code class="docutils literal notranslate"><span class="pre">SGD.step()</span></code></a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#sgGWR.optimizers.sg.SGDarmijo"><code class="docutils literal notranslate"><span class="pre">SGDarmijo</span></code></a><ul>
<li class="toctree-l4"><a class="reference internal" href="#sgGWR.optimizers.sg.SGDarmijo.armijo_cond"><code class="docutils literal notranslate"><span class="pre">SGDarmijo.armijo_cond()</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="#sgGWR.optimizers.sg.SGDarmijo.armijo_search"><code class="docutils literal notranslate"><span class="pre">SGDarmijo.armijo_search()</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="#sgGWR.optimizers.sg.SGDarmijo.lr_schedule"><code class="docutils literal notranslate"><span class="pre">SGDarmijo.lr_schedule()</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="#sgGWR.optimizers.sg.SGDarmijo.step"><code class="docutils literal notranslate"><span class="pre">SGDarmijo.step()</span></code></a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#sgGWR.optimizers.sg.Yogi"><code class="docutils literal notranslate"><span class="pre">Yogi</span></code></a><ul>
<li class="toctree-l4"><a class="reference internal" href="#sgGWR.optimizers.sg.Yogi.step"><code class="docutils literal notranslate"><span class="pre">Yogi.step()</span></code></a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#module-sgGWR.optimizers.sg_numpy">sgGWR.optimizers.sg_numpy module</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#sgGWR.optimizers.sg_numpy.ASGD"><code class="docutils literal notranslate"><span class="pre">ASGD</span></code></a><ul>
<li class="toctree-l4"><a class="reference internal" href="#sgGWR.optimizers.sg_numpy.ASGD.lr_schedule"><code class="docutils literal notranslate"><span class="pre">ASGD.lr_schedule()</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="#sgGWR.optimizers.sg_numpy.ASGD.step"><code class="docutils literal notranslate"><span class="pre">ASGD.step()</span></code></a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#sgGWR.optimizers.sg_numpy.SGD"><code class="docutils literal notranslate"><span class="pre">SGD</span></code></a><ul>
<li class="toctree-l4"><a class="reference internal" href="#sgGWR.optimizers.sg_numpy.SGD.lr_schedule"><code class="docutils literal notranslate"><span class="pre">SGD.lr_schedule()</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="#sgGWR.optimizers.sg_numpy.SGD.run"><code class="docutils literal notranslate"><span class="pre">SGD.run()</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="#sgGWR.optimizers.sg_numpy.SGD.step"><code class="docutils literal notranslate"><span class="pre">SGD.step()</span></code></a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#sgGWR.optimizers.sg_numpy.SGDarmijo"><code class="docutils literal notranslate"><span class="pre">SGDarmijo</span></code></a><ul>
<li class="toctree-l4"><a class="reference internal" href="#sgGWR.optimizers.sg_numpy.SGDarmijo.armijo_cond"><code class="docutils literal notranslate"><span class="pre">SGDarmijo.armijo_cond()</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="#sgGWR.optimizers.sg_numpy.SGDarmijo.armijo_search"><code class="docutils literal notranslate"><span class="pre">SGDarmijo.armijo_search()</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="#sgGWR.optimizers.sg_numpy.SGDarmijo.lr_schedule"><code class="docutils literal notranslate"><span class="pre">SGDarmijo.lr_schedule()</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="#sgGWR.optimizers.sg_numpy.SGDarmijo.step"><code class="docutils literal notranslate"><span class="pre">SGDarmijo.step()</span></code></a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#module-sgGWR.optimizers.vr">sgGWR.optimizers.vr module</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#sgGWR.optimizers.vr.KatyushaXs"><code class="docutils literal notranslate"><span class="pre">KatyushaXs</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="#sgGWR.optimizers.vr.KatyushaXw"><code class="docutils literal notranslate"><span class="pre">KatyushaXw</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="#sgGWR.optimizers.vr.SVRG"><code class="docutils literal notranslate"><span class="pre">SVRG</span></code></a><ul>
<li class="toctree-l4"><a class="reference internal" href="#sgGWR.optimizers.vr.SVRG.batch_step"><code class="docutils literal notranslate"><span class="pre">SVRG.batch_step()</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="#sgGWR.optimizers.vr.SVRG.run"><code class="docutils literal notranslate"><span class="pre">SVRG.run()</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="#sgGWR.optimizers.vr.SVRG.step"><code class="docutils literal notranslate"><span class="pre">SVRG.step()</span></code></a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#module-sgGWR.optimizers">Module contents</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#sgGWR.optimizers.ASGD"><code class="docutils literal notranslate"><span class="pre">ASGD</span></code></a><ul>
<li class="toctree-l4"><a class="reference internal" href="#sgGWR.optimizers.ASGD.lr_schedule"><code class="docutils literal notranslate"><span class="pre">ASGD.lr_schedule()</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="#sgGWR.optimizers.ASGD.step"><code class="docutils literal notranslate"><span class="pre">ASGD.step()</span></code></a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#sgGWR.optimizers.Adam"><code class="docutils literal notranslate"><span class="pre">Adam</span></code></a><ul>
<li class="toctree-l4"><a class="reference internal" href="#sgGWR.optimizers.Adam.step"><code class="docutils literal notranslate"><span class="pre">Adam.step()</span></code></a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#sgGWR.optimizers.KatyushaXs"><code class="docutils literal notranslate"><span class="pre">KatyushaXs</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="#sgGWR.optimizers.KatyushaXw"><code class="docutils literal notranslate"><span class="pre">KatyushaXw</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="#sgGWR.optimizers.SGD"><code class="docutils literal notranslate"><span class="pre">SGD</span></code></a><ul>
<li class="toctree-l4"><a class="reference internal" href="#sgGWR.optimizers.SGD.lr_schedule"><code class="docutils literal notranslate"><span class="pre">SGD.lr_schedule()</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="#sgGWR.optimizers.SGD.run"><code class="docutils literal notranslate"><span class="pre">SGD.run()</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="#sgGWR.optimizers.SGD.step"><code class="docutils literal notranslate"><span class="pre">SGD.step()</span></code></a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#sgGWR.optimizers.SGDarmijo"><code class="docutils literal notranslate"><span class="pre">SGDarmijo</span></code></a><ul>
<li class="toctree-l4"><a class="reference internal" href="#sgGWR.optimizers.SGDarmijo.armijo_cond"><code class="docutils literal notranslate"><span class="pre">SGDarmijo.armijo_cond()</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="#sgGWR.optimizers.SGDarmijo.armijo_search"><code class="docutils literal notranslate"><span class="pre">SGDarmijo.armijo_search()</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="#sgGWR.optimizers.SGDarmijo.lr_schedule"><code class="docutils literal notranslate"><span class="pre">SGDarmijo.lr_schedule()</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="#sgGWR.optimizers.SGDarmijo.step"><code class="docutils literal notranslate"><span class="pre">SGDarmijo.step()</span></code></a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#sgGWR.optimizers.SGN"><code class="docutils literal notranslate"><span class="pre">SGN</span></code></a><ul>
<li class="toctree-l4"><a class="reference internal" href="#sgGWR.optimizers.SGN.run"><code class="docutils literal notranslate"><span class="pre">SGN.run()</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="#sgGWR.optimizers.SGN.step"><code class="docutils literal notranslate"><span class="pre">SGN.step()</span></code></a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#sgGWR.optimizers.SGN_BFGS"><code class="docutils literal notranslate"><span class="pre">SGN_BFGS</span></code></a><ul>
<li class="toctree-l4"><a class="reference internal" href="#sgGWR.optimizers.SGN_BFGS.step"><code class="docutils literal notranslate"><span class="pre">SGN_BFGS.step()</span></code></a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#sgGWR.optimizers.SGN_LM"><code class="docutils literal notranslate"><span class="pre">SGN_LM</span></code></a><ul>
<li class="toctree-l4"><a class="reference internal" href="#sgGWR.optimizers.SGN_LM.step"><code class="docutils literal notranslate"><span class="pre">SGN_LM.step()</span></code></a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#sgGWR.optimizers.SVRG"><code class="docutils literal notranslate"><span class="pre">SVRG</span></code></a><ul>
<li class="toctree-l4"><a class="reference internal" href="#sgGWR.optimizers.SVRG.batch_step"><code class="docutils literal notranslate"><span class="pre">SVRG.batch_step()</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="#sgGWR.optimizers.SVRG.run"><code class="docutils literal notranslate"><span class="pre">SVRG.run()</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="#sgGWR.optimizers.SVRG.step"><code class="docutils literal notranslate"><span class="pre">SVRG.step()</span></code></a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#sgGWR.optimizers.Yogi"><code class="docutils literal notranslate"><span class="pre">Yogi</span></code></a><ul>
<li class="toctree-l4"><a class="reference internal" href="#sgGWR.optimizers.Yogi.step"><code class="docutils literal notranslate"><span class="pre">Yogi.step()</span></code></a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#sgGWR.optimizers.optax_optimizer"><code class="docutils literal notranslate"><span class="pre">optax_optimizer</span></code></a><ul>
<li class="toctree-l4"><a class="reference internal" href="#sgGWR.optimizers.optax_optimizer.run"><code class="docutils literal notranslate"><span class="pre">optax_optimizer.run()</span></code></a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#sgGWR.optimizers.scipy_L_BFGS_B"><code class="docutils literal notranslate"><span class="pre">scipy_L_BFGS_B</span></code></a><ul>
<li class="toctree-l4"><a class="reference internal" href="#sgGWR.optimizers.scipy_L_BFGS_B.run"><code class="docutils literal notranslate"><span class="pre">scipy_L_BFGS_B.run()</span></code></a></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="index.html">sgGWR</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="index.html" class="icon icon-home" aria-label="Home"></a></li>
          <li class="breadcrumb-item"><a href="sgGWR.html">sgGWR package</a></li>
      <li class="breadcrumb-item active">sgGWR.optimizers package</li>
      <li class="wy-breadcrumbs-aside">
            <a href="_sources/sgGWR.optimizers.rst.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <section id="sggwr-optimizers-package">
<h1>sgGWR.optimizers package<a class="headerlink" href="#sggwr-optimizers-package" title="Permalink to this heading"></a></h1>
<section id="submodules">
<h2>Submodules<a class="headerlink" href="#submodules" title="Permalink to this heading"></a></h2>
</section>
<section id="module-sgGWR.optimizers.existings">
<span id="sggwr-optimizers-existings-module"></span><h2>sgGWR.optimizers.existings module<a class="headerlink" href="#module-sgGWR.optimizers.existings" title="Permalink to this heading"></a></h2>
<p>Optimizers using other packages. Currently we support optimizers in scipy and optax.</p>
<dl class="py class">
<dt class="sig sig-object py" id="sgGWR.optimizers.existings.optax_optimizer">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">sgGWR.optimizers.existings.</span></span><span class="sig-name descname"><span class="pre">optax_optimizer</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">optax_optim=GradientTransformation(init=&lt;function</span> <span class="pre">chain.&lt;locals&gt;.init_fn&gt;</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">update=&lt;function</span> <span class="pre">chain.&lt;locals&gt;.update_fn&gt;)</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#sgGWR.optimizers.existings.optax_optimizer" title="Permalink to this definition"></a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">object</span></code></p>
<dl class="py method">
<dt class="sig sig-object py" id="sgGWR.optimizers.existings.optax_optimizer.run">
<span class="sig-name descname"><span class="pre">run</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">model</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">maxiter</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1000</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">batchsize</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">100</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">PRNGkey</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">diff_mode</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'manual'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">tol</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.001</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">n_iter_no_change</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">100</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">verbose</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#sgGWR.optimizers.existings.optax_optimizer.run" title="Permalink to this definition"></a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="sgGWR.optimizers.existings.scipy_L_BFGS_B">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">sgGWR.optimizers.existings.</span></span><span class="sig-name descname"><span class="pre">scipy_L_BFGS_B</span></span><a class="headerlink" href="#sgGWR.optimizers.existings.scipy_L_BFGS_B" title="Permalink to this definition"></a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">object</span></code></p>
<p>same setting to the ‘scgwr’ package in R
see:  <a class="reference external" href="https://github.com/cran/scgwr/blob/master/R/scgwr.R">https://github.com/cran/scgwr/blob/master/R/scgwr.R</a></p>
<dl class="py method">
<dt class="sig sig-object py" id="sgGWR.optimizers.existings.scipy_L_BFGS_B.run">
<span class="sig-name descname"><span class="pre">run</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">model</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">diff_mode</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'auto'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">tol</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">kwargs_minimize</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">{}</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#sgGWR.optimizers.existings.scipy_L_BFGS_B.run" title="Permalink to this definition"></a></dt>
<dd></dd></dl>

</dd></dl>

</section>
<section id="module-sgGWR.optimizers.existings_numpy">
<span id="sggwr-optimizers-existings-numpy-module"></span><h2>sgGWR.optimizers.existings_numpy module<a class="headerlink" href="#module-sgGWR.optimizers.existings_numpy" title="Permalink to this heading"></a></h2>
<p>Optimizers using other packages. Currently we support optimizers in scipy.</p>
<dl class="py class">
<dt class="sig sig-object py" id="sgGWR.optimizers.existings_numpy.scipy_L_BFGS_B">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">sgGWR.optimizers.existings_numpy.</span></span><span class="sig-name descname"><span class="pre">scipy_L_BFGS_B</span></span><a class="headerlink" href="#sgGWR.optimizers.existings_numpy.scipy_L_BFGS_B" title="Permalink to this definition"></a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">object</span></code></p>
<p>same setting to the ‘scgwr’ package in R
see:  <a class="reference external" href="https://github.com/cran/scgwr/blob/master/R/scgwr.R">https://github.com/cran/scgwr/blob/master/R/scgwr.R</a></p>
<dl class="py method">
<dt class="sig sig-object py" id="sgGWR.optimizers.existings_numpy.scipy_L_BFGS_B.run">
<span class="sig-name descname"><span class="pre">run</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">model</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">diff_mode</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'manual'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">tol</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">kwargs_minimize</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">{}</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#sgGWR.optimizers.existings_numpy.scipy_L_BFGS_B.run" title="Permalink to this definition"></a></dt>
<dd></dd></dl>

</dd></dl>

</section>
<section id="module-sgGWR.optimizers.second">
<span id="sggwr-optimizers-second-module"></span><h2>sgGWR.optimizers.second module<a class="headerlink" href="#module-sgGWR.optimizers.second" title="Permalink to this heading"></a></h2>
<p>Optimizers using stochastic gradients with second order informations.
Faster convergence can be expected.</p>
<dl class="py class">
<dt class="sig sig-object py" id="sgGWR.optimizers.second.SGN">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">sgGWR.optimizers.second.</span></span><span class="sig-name descname"><span class="pre">SGN</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">learning_rate0</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1.0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">lam</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#sgGWR.optimizers.second.SGN" title="Permalink to this definition"></a></dt>
<dd><p>Bases: <a class="reference internal" href="#sgGWR.optimizers.sg.SGD" title="sgGWR.optimizers.sg.SGD"><code class="xref py py-class docutils literal notranslate"><span class="pre">SGD</span></code></a></p>
<p>Stochastic Gauss-Newton methods.
Small value is added to approximated Hessian to guarantee its positive definitness.</p>
<p>refernces:
Bottou, L., Curtis, F. E., &amp; Nocedal, J. (2018).
Optimization methods for large-scale machine learning.
In SIAM Review (Vol. 60, Issue 2, pp. 223–311).</p>
<dl class="py method">
<dt class="sig sig-object py" id="sgGWR.optimizers.second.SGN.run">
<span class="sig-name descname"><span class="pre">run</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">model</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">maxiter</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1000</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">batchsize</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">100</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">PRNGkey</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">Array([0,</span> <span class="pre">123],</span> <span class="pre">dtype=uint32)</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">diff_mode</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'manual'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">tol</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.001</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">n_iter_no_change</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">100</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">verbose</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#sgGWR.optimizers.second.SGN.run" title="Permalink to this definition"></a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="sgGWR.optimizers.second.SGN.step">
<span class="sig-name descname"><span class="pre">step</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">t</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">x</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">f_g_J</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">idx</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#sgGWR.optimizers.second.SGN.step" title="Permalink to this definition"></a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="sgGWR.optimizers.second.SGN_BFGS">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">sgGWR.optimizers.second.</span></span><span class="sig-name descname"><span class="pre">SGN_BFGS</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">learning_rate0</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1.0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">lam</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#sgGWR.optimizers.second.SGN_BFGS" title="Permalink to this definition"></a></dt>
<dd><p>Bases: <a class="reference internal" href="#sgGWR.optimizers.second.SGN" title="sgGWR.optimizers.second.SGN"><code class="xref py py-class docutils literal notranslate"><span class="pre">SGN</span></code></a></p>
<p>Stochastic Gauss-Newton methods.
BFGS formula is applied to guarantee the positive definiteness of approximated Hessian matrix.
Note: I recommend small learning rate</p>
<p>refernces:
Bottou, L., Curtis, F. E., &amp; Nocedal, J. (2018).
Optimization methods for large-scale machine learning.
In SIAM Review (Vol. 60, Issue 2, pp. 223–311).</p>
<dl class="py method">
<dt class="sig sig-object py" id="sgGWR.optimizers.second.SGN_BFGS.step">
<span class="sig-name descname"><span class="pre">step</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">t</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">x</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">f_g_J</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">idx</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#sgGWR.optimizers.second.SGN_BFGS.step" title="Permalink to this definition"></a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="sgGWR.optimizers.second.SGN_LM">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">sgGWR.optimizers.second.</span></span><span class="sig-name descname"><span class="pre">SGN_LM</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">learning_rate0</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1.0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">lam</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">lam_LM0</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1.0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">boost</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1.01</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">drop</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.99</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">eps</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.25</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">tau</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.001</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#sgGWR.optimizers.second.SGN_LM" title="Permalink to this definition"></a></dt>
<dd><p>Bases: <a class="reference internal" href="#sgGWR.optimizers.second.SGN" title="sgGWR.optimizers.second.SGN"><code class="xref py py-class docutils literal notranslate"><span class="pre">SGN</span></code></a></p>
<p>Stochastic Gauss-Newton methods with damping of Levenberg-Marquardt (LM) method.
The damping improves the stability.
This algorithm refered to as “SMW-GN” in Ren and Goldfarb (2019)
because they used Sherman-Morrison-Woodbury(SMW) formula.
SMW formula is not used in this implementation
because we assume that the number of parameters is not far larger than the mini-batch size.</p>
<p>refernces:
Ren, Y., &amp; Goldfarb, D. (2019).
Efficient Subsampled Gauss-Newton and Natural Gradient Methods for Training Neural Networks.
<a class="reference external" href="https://arxiv.org/abs/1906.02353v1">https://arxiv.org/abs/1906.02353v1</a></p>
<dl class="py method">
<dt class="sig sig-object py" id="sgGWR.optimizers.second.SGN_LM.step">
<span class="sig-name descname"><span class="pre">step</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">t</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">x</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">f_g_J</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">idx</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#sgGWR.optimizers.second.SGN_LM.step" title="Permalink to this definition"></a></dt>
<dd></dd></dl>

</dd></dl>

</section>
<section id="module-sgGWR.optimizers.sg">
<span id="sggwr-optimizers-sg-module"></span><h2>sgGWR.optimizers.sg module<a class="headerlink" href="#module-sgGWR.optimizers.sg" title="Permalink to this heading"></a></h2>
<p>Optimizers using stochastic gradients.</p>
<dl class="py class">
<dt class="sig sig-object py" id="sgGWR.optimizers.sg.ASGD">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">sgGWR.optimizers.sg.</span></span><span class="sig-name descname"><span class="pre">ASGD</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">learning_rate0</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1.0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">lam</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.0001</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#sgGWR.optimizers.sg.ASGD" title="Permalink to this definition"></a></dt>
<dd><p>Bases: <a class="reference internal" href="#sgGWR.optimizers.sg.SGD" title="sgGWR.optimizers.sg.SGD"><code class="xref py py-class docutils literal notranslate"><span class="pre">SGD</span></code></a></p>
<p>Avereaged Stochastic Gradient Descent Algorithm</p>
<p>reference:
Bottou, L. (2010). Large-scale machine learning with stochastic gradient descent.
Proceedings of COMPSTAT 2010 - 19th International Conference on Computational Statistics,
Keynote, Invited and Contributed Papers, 177–186. <a class="reference external" href="https://doi.org/10.1007/978-3-7908-2604-3_16">https://doi.org/10.1007/978-3-7908-2604-3_16</a></p>
<dl class="py method">
<dt class="sig sig-object py" id="sgGWR.optimizers.sg.ASGD.lr_schedule">
<span class="sig-name descname"><span class="pre">lr_schedule</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">t</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#sgGWR.optimizers.sg.ASGD.lr_schedule" title="Permalink to this definition"></a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="sgGWR.optimizers.sg.ASGD.step">
<span class="sig-name descname"><span class="pre">step</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">t</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">x</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">f</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">g</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">f_and_g</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">idx</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#sgGWR.optimizers.sg.ASGD.step" title="Permalink to this definition"></a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="sgGWR.optimizers.sg.Adam">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">sgGWR.optimizers.sg.</span></span><span class="sig-name descname"><span class="pre">Adam</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">learning_rate0</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1.0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">b1</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.9</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">b2</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.999</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">eps</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1e-08</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">correct_bias</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#sgGWR.optimizers.sg.Adam" title="Permalink to this definition"></a></dt>
<dd><p>Bases: <a class="reference internal" href="#sgGWR.optimizers.sg.SGD" title="sgGWR.optimizers.sg.SGD"><code class="xref py py-class docutils literal notranslate"><span class="pre">SGD</span></code></a></p>
<dl class="py method">
<dt class="sig sig-object py" id="sgGWR.optimizers.sg.Adam.step">
<span class="sig-name descname"><span class="pre">step</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">t</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">x</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">f</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">g</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">f_and_g</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">idx</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#sgGWR.optimizers.sg.Adam.step" title="Permalink to this definition"></a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="sgGWR.optimizers.sg.SGD">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">sgGWR.optimizers.sg.</span></span><span class="sig-name descname"><span class="pre">SGD</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">learning_rate0</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1.0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">lam</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.0001</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#sgGWR.optimizers.sg.SGD" title="Permalink to this definition"></a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">object</span></code></p>
<p>Stochastic Gradient Descent Algorithm</p>
<p>reference:
Bottou, L. (2010). Large-scale machine learning with stochastic gradient descent.
Proceedings of COMPSTAT 2010 - 19th International Conference on Computational Statistics,
Keynote, Invited and Contributed Papers, 177–186. <a class="reference external" href="https://doi.org/10.1007/978-3-7908-2604-3_16">https://doi.org/10.1007/978-3-7908-2604-3_16</a></p>
<dl class="py method">
<dt class="sig sig-object py" id="sgGWR.optimizers.sg.SGD.lr_schedule">
<span class="sig-name descname"><span class="pre">lr_schedule</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">t</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#sgGWR.optimizers.sg.SGD.lr_schedule" title="Permalink to this definition"></a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="sgGWR.optimizers.sg.SGD.run">
<span class="sig-name descname"><span class="pre">run</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">model</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">maxiter</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1000</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">batchsize</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">100</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">PRNGkey</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">Array([0,</span> <span class="pre">123],</span> <span class="pre">dtype=uint32)</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">diff_mode</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'manual'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">tol</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.001</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">n_iter_no_change</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">100</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">verbose</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#sgGWR.optimizers.sg.SGD.run" title="Permalink to this definition"></a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="sgGWR.optimizers.sg.SGD.step">
<span class="sig-name descname"><span class="pre">step</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">t</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">x</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">f</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">g</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">f_and_g</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">idx</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#sgGWR.optimizers.sg.SGD.step" title="Permalink to this definition"></a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="sgGWR.optimizers.sg.SGDarmijo">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">sgGWR.optimizers.sg.</span></span><span class="sig-name descname"><span class="pre">SGDarmijo</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">learning_rate0</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1.0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">c</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.5</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">ls_decay</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.5</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">reset_decay</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">2.0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">search_from_lr0</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#sgGWR.optimizers.sg.SGDarmijo" title="Permalink to this definition"></a></dt>
<dd><p>Bases: <a class="reference internal" href="#sgGWR.optimizers.sg.SGD" title="sgGWR.optimizers.sg.SGD"><code class="xref py py-class docutils literal notranslate"><span class="pre">SGD</span></code></a></p>
<p>Stochastic Gradient Descent Algorithm with Armijo Line-search</p>
<p>reference:
Vaswani, S., Mishkin, A., Laradji, I., Schmidt, M., Gidel, G., &amp; Lacoste-Julien, S. (2019).
Painless stochastic gradient: Interpolation, line-search, and convergence rates.
Advances in neural information processing systems, 32.</p>
<dl class="py method">
<dt class="sig sig-object py" id="sgGWR.optimizers.sg.SGDarmijo.armijo_cond">
<span class="sig-name descname"><span class="pre">armijo_cond</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">f</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">f0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">x</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">grads</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">g2</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">lr</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#sgGWR.optimizers.sg.SGDarmijo.armijo_cond" title="Permalink to this definition"></a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="sgGWR.optimizers.sg.SGDarmijo.armijo_search">
<span class="sig-name descname"><span class="pre">armijo_search</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">x</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">grads</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">lr</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">f</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#sgGWR.optimizers.sg.SGDarmijo.armijo_search" title="Permalink to this definition"></a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="sgGWR.optimizers.sg.SGDarmijo.lr_schedule">
<span class="sig-name descname"><span class="pre">lr_schedule</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">t</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#sgGWR.optimizers.sg.SGDarmijo.lr_schedule" title="Permalink to this definition"></a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="sgGWR.optimizers.sg.SGDarmijo.step">
<span class="sig-name descname"><span class="pre">step</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">t</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">x</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">f</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">g</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">f_and_g</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">idx</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#sgGWR.optimizers.sg.SGDarmijo.step" title="Permalink to this definition"></a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="sgGWR.optimizers.sg.Yogi">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">sgGWR.optimizers.sg.</span></span><span class="sig-name descname"><span class="pre">Yogi</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">learning_rate0</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1.0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">b1</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.9</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">b2</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.999</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">eps</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1e-08</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">correct_bias</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#sgGWR.optimizers.sg.Yogi" title="Permalink to this definition"></a></dt>
<dd><p>Bases: <a class="reference internal" href="#sgGWR.optimizers.sg.Adam" title="sgGWR.optimizers.sg.Adam"><code class="xref py py-class docutils literal notranslate"><span class="pre">Adam</span></code></a></p>
<dl class="py method">
<dt class="sig sig-object py" id="sgGWR.optimizers.sg.Yogi.step">
<span class="sig-name descname"><span class="pre">step</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">t</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">x</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">f</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">g</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">f_and_g</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">idx</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#sgGWR.optimizers.sg.Yogi.step" title="Permalink to this definition"></a></dt>
<dd></dd></dl>

</dd></dl>

</section>
<section id="module-sgGWR.optimizers.sg_numpy">
<span id="sggwr-optimizers-sg-numpy-module"></span><h2>sgGWR.optimizers.sg_numpy module<a class="headerlink" href="#module-sgGWR.optimizers.sg_numpy" title="Permalink to this heading"></a></h2>
<p>Optimizers using stochastic gradients.</p>
<dl class="py class">
<dt class="sig sig-object py" id="sgGWR.optimizers.sg_numpy.ASGD">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">sgGWR.optimizers.sg_numpy.</span></span><span class="sig-name descname"><span class="pre">ASGD</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">learning_rate0</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">lam</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.0001</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#sgGWR.optimizers.sg_numpy.ASGD" title="Permalink to this definition"></a></dt>
<dd><p>Bases: <a class="reference internal" href="#sgGWR.optimizers.sg_numpy.SGD" title="sgGWR.optimizers.sg_numpy.SGD"><code class="xref py py-class docutils literal notranslate"><span class="pre">SGD</span></code></a></p>
<p>Avereaged Stochastic Gradient Descent Algorithm</p>
<p>reference:
Bottou, L. (2010). Large-scale machine learning with stochastic gradient descent.
Proceedings of COMPSTAT 2010 - 19th International Conference on Computational Statistics,
Keynote, Invited and Contributed Papers, 177–186. <a class="reference external" href="https://doi.org/10.1007/978-3-7908-2604-3_16">https://doi.org/10.1007/978-3-7908-2604-3_16</a></p>
<dl class="py method">
<dt class="sig sig-object py" id="sgGWR.optimizers.sg_numpy.ASGD.lr_schedule">
<span class="sig-name descname"><span class="pre">lr_schedule</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">t</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#sgGWR.optimizers.sg_numpy.ASGD.lr_schedule" title="Permalink to this definition"></a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="sgGWR.optimizers.sg_numpy.ASGD.step">
<span class="sig-name descname"><span class="pre">step</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">t</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">x</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">f</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">g</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">f_and_g</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">idx</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#sgGWR.optimizers.sg_numpy.ASGD.step" title="Permalink to this definition"></a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="sgGWR.optimizers.sg_numpy.SGD">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">sgGWR.optimizers.sg_numpy.</span></span><span class="sig-name descname"><span class="pre">SGD</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">learning_rate0</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">lam</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.0001</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#sgGWR.optimizers.sg_numpy.SGD" title="Permalink to this definition"></a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">object</span></code></p>
<p>Stochastic Gradient Descent Algorithm</p>
<p>reference:
Bottou, L. (2010). Large-scale machine learning with stochastic gradient descent.
Proceedings of COMPSTAT 2010 - 19th International Conference on Computational Statistics,
Keynote, Invited and Contributed Papers, 177–186. <a class="reference external" href="https://doi.org/10.1007/978-3-7908-2604-3_16">https://doi.org/10.1007/978-3-7908-2604-3_16</a></p>
<dl class="py method">
<dt class="sig sig-object py" id="sgGWR.optimizers.sg_numpy.SGD.lr_schedule">
<span class="sig-name descname"><span class="pre">lr_schedule</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">t</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#sgGWR.optimizers.sg_numpy.SGD.lr_schedule" title="Permalink to this definition"></a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="sgGWR.optimizers.sg_numpy.SGD.run">
<span class="sig-name descname"><span class="pre">run</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">model</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">maxiter=1000</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">batchsize=100</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">rng=Generator(PCG64)</span> <span class="pre">at</span> <span class="pre">0x7FD99FB0AD60</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">tol=0.001</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">n_iter_no_change=100</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">verbose=True</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#sgGWR.optimizers.sg_numpy.SGD.run" title="Permalink to this definition"></a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="sgGWR.optimizers.sg_numpy.SGD.step">
<span class="sig-name descname"><span class="pre">step</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">t</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">x</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">f</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">g</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">f_and_g</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">idx</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#sgGWR.optimizers.sg_numpy.SGD.step" title="Permalink to this definition"></a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="sgGWR.optimizers.sg_numpy.SGDarmijo">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">sgGWR.optimizers.sg_numpy.</span></span><span class="sig-name descname"><span class="pre">SGDarmijo</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">learning_rate0</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1.0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">c</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.5</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">ls_decay</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.5</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">reset_decay</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">2.0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">search_from_lr0</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#sgGWR.optimizers.sg_numpy.SGDarmijo" title="Permalink to this definition"></a></dt>
<dd><p>Bases: <a class="reference internal" href="#sgGWR.optimizers.sg_numpy.SGD" title="sgGWR.optimizers.sg_numpy.SGD"><code class="xref py py-class docutils literal notranslate"><span class="pre">SGD</span></code></a></p>
<p>Stochastic Gradient Descent Algorithm with Armijo Line-search</p>
<p>reference:
Vaswani, S., Mishkin, A., Laradji, I., Schmidt, M., Gidel, G., &amp; Lacoste-Julien, S. (2019).
Painless stochastic gradient: Interpolation, line-search, and convergence rates.
Advances in neural information processing systems, 32.</p>
<dl class="py method">
<dt class="sig sig-object py" id="sgGWR.optimizers.sg_numpy.SGDarmijo.armijo_cond">
<span class="sig-name descname"><span class="pre">armijo_cond</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">f</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">f0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">x</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">grads</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">g2</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">lr</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#sgGWR.optimizers.sg_numpy.SGDarmijo.armijo_cond" title="Permalink to this definition"></a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="sgGWR.optimizers.sg_numpy.SGDarmijo.armijo_search">
<span class="sig-name descname"><span class="pre">armijo_search</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">x</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">grads</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">lr</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">f</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#sgGWR.optimizers.sg_numpy.SGDarmijo.armijo_search" title="Permalink to this definition"></a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="sgGWR.optimizers.sg_numpy.SGDarmijo.lr_schedule">
<span class="sig-name descname"><span class="pre">lr_schedule</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">t</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#sgGWR.optimizers.sg_numpy.SGDarmijo.lr_schedule" title="Permalink to this definition"></a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="sgGWR.optimizers.sg_numpy.SGDarmijo.step">
<span class="sig-name descname"><span class="pre">step</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">t</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">x</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">f</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">g</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">f_and_g</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">idx</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#sgGWR.optimizers.sg_numpy.SGDarmijo.step" title="Permalink to this definition"></a></dt>
<dd></dd></dl>

</dd></dl>

</section>
<section id="module-sgGWR.optimizers.vr">
<span id="sggwr-optimizers-vr-module"></span><h2>sgGWR.optimizers.vr module<a class="headerlink" href="#module-sgGWR.optimizers.vr" title="Permalink to this heading"></a></h2>
<p>Optimizers using variance reduced stochastic gradients.
They are recommended if you require high-accuracy optimization.</p>
<dl class="py class">
<dt class="sig sig-object py" id="sgGWR.optimizers.vr.KatyushaXs">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">sgGWR.optimizers.vr.</span></span><span class="sig-name descname"><span class="pre">KatyushaXs</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">learning_rate0</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">lam</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">neg_moment</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.1</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#sgGWR.optimizers.vr.KatyushaXs" title="Permalink to this definition"></a></dt>
<dd><p>Bases: <a class="reference internal" href="#sgGWR.optimizers.vr.SVRG" title="sgGWR.optimizers.vr.SVRG"><code class="xref py py-class docutils literal notranslate"><span class="pre">SVRG</span></code></a></p>
<p>KatyushaXs algorithm</p>
<p>refernces:
Allen-Zhu, Z. (2018).
Katyusha X: Practical Momentum Method for Stochastic Sum-of-Nonconvex Optimization.
35th International Conference on Machine Learning, ICML 2018, 1, 284–290.</p>
</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="sgGWR.optimizers.vr.KatyushaXw">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">sgGWR.optimizers.vr.</span></span><span class="sig-name descname"><span class="pre">KatyushaXw</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">learning_rate0</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">lam</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#sgGWR.optimizers.vr.KatyushaXw" title="Permalink to this definition"></a></dt>
<dd><p>Bases: <a class="reference internal" href="#sgGWR.optimizers.vr.SVRG" title="sgGWR.optimizers.vr.SVRG"><code class="xref py py-class docutils literal notranslate"><span class="pre">SVRG</span></code></a></p>
<p>KatyushaXw algorithm</p>
<p>refernces:
Allen-Zhu, Z. (2018).
Katyusha X: Practical Momentum Method for Stochastic Sum-of-Nonconvex Optimization.
35th International Conference on Machine Learning, ICML 2018, 1, 284–290.</p>
</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="sgGWR.optimizers.vr.SVRG">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">sgGWR.optimizers.vr.</span></span><span class="sig-name descname"><span class="pre">SVRG</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">learning_rate0</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">lam</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#sgGWR.optimizers.vr.SVRG" title="Permalink to this definition"></a></dt>
<dd><p>Bases: <a class="reference internal" href="#sgGWR.optimizers.sg.SGD" title="sgGWR.optimizers.sg.SGD"><code class="xref py py-class docutils literal notranslate"><span class="pre">SGD</span></code></a></p>
<p>Stochastic Variance Reduced Gradient with mini-batch</p>
<p>refernces:
Johnson, R., &amp; Zhang, T. (2013).
Accelerating Stochastic Gradient Descent using Predictive Variance Reduction.
In Advances in Neural Information Processing Systems (Vol. 26).</p>
<p>Allen-Zhu, Z. (2018).
Katyusha X: Practical Momentum Method for Stochastic Sum-of-Nonconvex Optimization.
35th International Conference on Machine Learning, ICML 2018, 1, 284–290.</p>
<dl class="py method">
<dt class="sig sig-object py" id="sgGWR.optimizers.vr.SVRG.batch_step">
<span class="sig-name descname"><span class="pre">batch_step</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">f_step</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">x0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">idxs</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#sgGWR.optimizers.vr.SVRG.batch_step" title="Permalink to this definition"></a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="sgGWR.optimizers.vr.SVRG.run">
<span class="sig-name descname"><span class="pre">run</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">model</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">max_epoch</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">100</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">batchsize</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">PRNGkey</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">Array([0,</span> <span class="pre">123],</span> <span class="pre">dtype=uint32)</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">diff_mode</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'manual'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">tol</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.001</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">n_iter_no_change</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">5</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">min_epoch</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">5</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">verbose</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">lax_scan</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#sgGWR.optimizers.vr.SVRG.run" title="Permalink to this definition"></a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="sgGWR.optimizers.vr.SVRG.step">
<span class="sig-name descname"><span class="pre">step</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">t</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">x</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">f</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">g</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">f_and_g</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">idx</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#sgGWR.optimizers.vr.SVRG.step" title="Permalink to this definition"></a></dt>
<dd></dd></dl>

</dd></dl>

</section>
<section id="module-sgGWR.optimizers">
<span id="module-contents"></span><h2>Module contents<a class="headerlink" href="#module-sgGWR.optimizers" title="Permalink to this heading"></a></h2>
<dl class="py class">
<dt class="sig sig-object py" id="sgGWR.optimizers.ASGD">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">sgGWR.optimizers.</span></span><span class="sig-name descname"><span class="pre">ASGD</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">learning_rate0</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1.0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">lam</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.0001</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#sgGWR.optimizers.ASGD" title="Permalink to this definition"></a></dt>
<dd><p>Bases: <a class="reference internal" href="#sgGWR.optimizers.sg.SGD" title="sgGWR.optimizers.sg.SGD"><code class="xref py py-class docutils literal notranslate"><span class="pre">SGD</span></code></a></p>
<p>Avereaged Stochastic Gradient Descent Algorithm</p>
<p>reference:
Bottou, L. (2010). Large-scale machine learning with stochastic gradient descent.
Proceedings of COMPSTAT 2010 - 19th International Conference on Computational Statistics,
Keynote, Invited and Contributed Papers, 177–186. <a class="reference external" href="https://doi.org/10.1007/978-3-7908-2604-3_16">https://doi.org/10.1007/978-3-7908-2604-3_16</a></p>
<dl class="py method">
<dt class="sig sig-object py" id="sgGWR.optimizers.ASGD.lr_schedule">
<span class="sig-name descname"><span class="pre">lr_schedule</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">t</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#sgGWR.optimizers.ASGD.lr_schedule" title="Permalink to this definition"></a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="sgGWR.optimizers.ASGD.step">
<span class="sig-name descname"><span class="pre">step</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">t</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">x</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">f</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">g</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">f_and_g</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">idx</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#sgGWR.optimizers.ASGD.step" title="Permalink to this definition"></a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="sgGWR.optimizers.Adam">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">sgGWR.optimizers.</span></span><span class="sig-name descname"><span class="pre">Adam</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">learning_rate0</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1.0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">b1</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.9</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">b2</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.999</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">eps</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1e-08</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">correct_bias</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#sgGWR.optimizers.Adam" title="Permalink to this definition"></a></dt>
<dd><p>Bases: <a class="reference internal" href="#sgGWR.optimizers.sg.SGD" title="sgGWR.optimizers.sg.SGD"><code class="xref py py-class docutils literal notranslate"><span class="pre">SGD</span></code></a></p>
<dl class="py method">
<dt class="sig sig-object py" id="sgGWR.optimizers.Adam.step">
<span class="sig-name descname"><span class="pre">step</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">t</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">x</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">f</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">g</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">f_and_g</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">idx</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#sgGWR.optimizers.Adam.step" title="Permalink to this definition"></a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="sgGWR.optimizers.KatyushaXs">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">sgGWR.optimizers.</span></span><span class="sig-name descname"><span class="pre">KatyushaXs</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">learning_rate0</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">lam</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">neg_moment</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.1</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#sgGWR.optimizers.KatyushaXs" title="Permalink to this definition"></a></dt>
<dd><p>Bases: <a class="reference internal" href="#sgGWR.optimizers.vr.SVRG" title="sgGWR.optimizers.vr.SVRG"><code class="xref py py-class docutils literal notranslate"><span class="pre">SVRG</span></code></a></p>
<p>KatyushaXs algorithm</p>
<p>refernces:
Allen-Zhu, Z. (2018).
Katyusha X: Practical Momentum Method for Stochastic Sum-of-Nonconvex Optimization.
35th International Conference on Machine Learning, ICML 2018, 1, 284–290.</p>
</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="sgGWR.optimizers.KatyushaXw">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">sgGWR.optimizers.</span></span><span class="sig-name descname"><span class="pre">KatyushaXw</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">learning_rate0</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">lam</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#sgGWR.optimizers.KatyushaXw" title="Permalink to this definition"></a></dt>
<dd><p>Bases: <a class="reference internal" href="#sgGWR.optimizers.vr.SVRG" title="sgGWR.optimizers.vr.SVRG"><code class="xref py py-class docutils literal notranslate"><span class="pre">SVRG</span></code></a></p>
<p>KatyushaXw algorithm</p>
<p>refernces:
Allen-Zhu, Z. (2018).
Katyusha X: Practical Momentum Method for Stochastic Sum-of-Nonconvex Optimization.
35th International Conference on Machine Learning, ICML 2018, 1, 284–290.</p>
</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="sgGWR.optimizers.SGD">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">sgGWR.optimizers.</span></span><span class="sig-name descname"><span class="pre">SGD</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">learning_rate0</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1.0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">lam</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.0001</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#sgGWR.optimizers.SGD" title="Permalink to this definition"></a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">object</span></code></p>
<p>Stochastic Gradient Descent Algorithm</p>
<p>reference:
Bottou, L. (2010). Large-scale machine learning with stochastic gradient descent.
Proceedings of COMPSTAT 2010 - 19th International Conference on Computational Statistics,
Keynote, Invited and Contributed Papers, 177–186. <a class="reference external" href="https://doi.org/10.1007/978-3-7908-2604-3_16">https://doi.org/10.1007/978-3-7908-2604-3_16</a></p>
<dl class="py method">
<dt class="sig sig-object py" id="sgGWR.optimizers.SGD.lr_schedule">
<span class="sig-name descname"><span class="pre">lr_schedule</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">t</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#sgGWR.optimizers.SGD.lr_schedule" title="Permalink to this definition"></a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="sgGWR.optimizers.SGD.run">
<span class="sig-name descname"><span class="pre">run</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">model</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">maxiter</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1000</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">batchsize</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">100</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">PRNGkey</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">Array([0,</span> <span class="pre">123],</span> <span class="pre">dtype=uint32)</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">diff_mode</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'manual'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">tol</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.001</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">n_iter_no_change</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">100</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">verbose</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#sgGWR.optimizers.SGD.run" title="Permalink to this definition"></a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="sgGWR.optimizers.SGD.step">
<span class="sig-name descname"><span class="pre">step</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">t</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">x</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">f</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">g</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">f_and_g</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">idx</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#sgGWR.optimizers.SGD.step" title="Permalink to this definition"></a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="sgGWR.optimizers.SGDarmijo">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">sgGWR.optimizers.</span></span><span class="sig-name descname"><span class="pre">SGDarmijo</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">learning_rate0</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1.0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">c</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.5</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">ls_decay</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.5</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">reset_decay</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">2.0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">search_from_lr0</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#sgGWR.optimizers.SGDarmijo" title="Permalink to this definition"></a></dt>
<dd><p>Bases: <a class="reference internal" href="#sgGWR.optimizers.sg.SGD" title="sgGWR.optimizers.sg.SGD"><code class="xref py py-class docutils literal notranslate"><span class="pre">SGD</span></code></a></p>
<p>Stochastic Gradient Descent Algorithm with Armijo Line-search</p>
<p>reference:
Vaswani, S., Mishkin, A., Laradji, I., Schmidt, M., Gidel, G., &amp; Lacoste-Julien, S. (2019).
Painless stochastic gradient: Interpolation, line-search, and convergence rates.
Advances in neural information processing systems, 32.</p>
<dl class="py method">
<dt class="sig sig-object py" id="sgGWR.optimizers.SGDarmijo.armijo_cond">
<span class="sig-name descname"><span class="pre">armijo_cond</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">f</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">f0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">x</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">grads</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">g2</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">lr</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#sgGWR.optimizers.SGDarmijo.armijo_cond" title="Permalink to this definition"></a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="sgGWR.optimizers.SGDarmijo.armijo_search">
<span class="sig-name descname"><span class="pre">armijo_search</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">x</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">grads</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">lr</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">f</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#sgGWR.optimizers.SGDarmijo.armijo_search" title="Permalink to this definition"></a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="sgGWR.optimizers.SGDarmijo.lr_schedule">
<span class="sig-name descname"><span class="pre">lr_schedule</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">t</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#sgGWR.optimizers.SGDarmijo.lr_schedule" title="Permalink to this definition"></a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="sgGWR.optimizers.SGDarmijo.step">
<span class="sig-name descname"><span class="pre">step</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">t</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">x</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">f</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">g</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">f_and_g</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">idx</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#sgGWR.optimizers.SGDarmijo.step" title="Permalink to this definition"></a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="sgGWR.optimizers.SGN">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">sgGWR.optimizers.</span></span><span class="sig-name descname"><span class="pre">SGN</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">learning_rate0</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1.0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">lam</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#sgGWR.optimizers.SGN" title="Permalink to this definition"></a></dt>
<dd><p>Bases: <a class="reference internal" href="#sgGWR.optimizers.sg.SGD" title="sgGWR.optimizers.sg.SGD"><code class="xref py py-class docutils literal notranslate"><span class="pre">SGD</span></code></a></p>
<p>Stochastic Gauss-Newton methods.
Small value is added to approximated Hessian to guarantee its positive definitness.</p>
<p>refernces:
Bottou, L., Curtis, F. E., &amp; Nocedal, J. (2018).
Optimization methods for large-scale machine learning.
In SIAM Review (Vol. 60, Issue 2, pp. 223–311).</p>
<dl class="py method">
<dt class="sig sig-object py" id="sgGWR.optimizers.SGN.run">
<span class="sig-name descname"><span class="pre">run</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">model</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">maxiter</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1000</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">batchsize</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">100</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">PRNGkey</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">Array([0,</span> <span class="pre">123],</span> <span class="pre">dtype=uint32)</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">diff_mode</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'manual'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">tol</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.001</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">n_iter_no_change</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">100</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">verbose</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#sgGWR.optimizers.SGN.run" title="Permalink to this definition"></a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="sgGWR.optimizers.SGN.step">
<span class="sig-name descname"><span class="pre">step</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">t</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">x</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">f_g_J</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">idx</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#sgGWR.optimizers.SGN.step" title="Permalink to this definition"></a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="sgGWR.optimizers.SGN_BFGS">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">sgGWR.optimizers.</span></span><span class="sig-name descname"><span class="pre">SGN_BFGS</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">learning_rate0</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1.0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">lam</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#sgGWR.optimizers.SGN_BFGS" title="Permalink to this definition"></a></dt>
<dd><p>Bases: <a class="reference internal" href="#sgGWR.optimizers.second.SGN" title="sgGWR.optimizers.second.SGN"><code class="xref py py-class docutils literal notranslate"><span class="pre">SGN</span></code></a></p>
<p>Stochastic Gauss-Newton methods.
BFGS formula is applied to guarantee the positive definiteness of approximated Hessian matrix.
Note: I recommend small learning rate</p>
<p>refernces:
Bottou, L., Curtis, F. E., &amp; Nocedal, J. (2018).
Optimization methods for large-scale machine learning.
In SIAM Review (Vol. 60, Issue 2, pp. 223–311).</p>
<dl class="py method">
<dt class="sig sig-object py" id="sgGWR.optimizers.SGN_BFGS.step">
<span class="sig-name descname"><span class="pre">step</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">t</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">x</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">f_g_J</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">idx</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#sgGWR.optimizers.SGN_BFGS.step" title="Permalink to this definition"></a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="sgGWR.optimizers.SGN_LM">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">sgGWR.optimizers.</span></span><span class="sig-name descname"><span class="pre">SGN_LM</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">learning_rate0</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1.0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">lam</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">lam_LM0</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1.0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">boost</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1.01</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">drop</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.99</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">eps</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.25</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">tau</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.001</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#sgGWR.optimizers.SGN_LM" title="Permalink to this definition"></a></dt>
<dd><p>Bases: <a class="reference internal" href="#sgGWR.optimizers.second.SGN" title="sgGWR.optimizers.second.SGN"><code class="xref py py-class docutils literal notranslate"><span class="pre">SGN</span></code></a></p>
<p>Stochastic Gauss-Newton methods with damping of Levenberg-Marquardt (LM) method.
The damping improves the stability.
This algorithm refered to as “SMW-GN” in Ren and Goldfarb (2019)
because they used Sherman-Morrison-Woodbury(SMW) formula.
SMW formula is not used in this implementation
because we assume that the number of parameters is not far larger than the mini-batch size.</p>
<p>refernces:
Ren, Y., &amp; Goldfarb, D. (2019).
Efficient Subsampled Gauss-Newton and Natural Gradient Methods for Training Neural Networks.
<a class="reference external" href="https://arxiv.org/abs/1906.02353v1">https://arxiv.org/abs/1906.02353v1</a></p>
<dl class="py method">
<dt class="sig sig-object py" id="sgGWR.optimizers.SGN_LM.step">
<span class="sig-name descname"><span class="pre">step</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">t</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">x</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">f_g_J</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">idx</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#sgGWR.optimizers.SGN_LM.step" title="Permalink to this definition"></a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="sgGWR.optimizers.SVRG">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">sgGWR.optimizers.</span></span><span class="sig-name descname"><span class="pre">SVRG</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">learning_rate0</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">lam</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#sgGWR.optimizers.SVRG" title="Permalink to this definition"></a></dt>
<dd><p>Bases: <a class="reference internal" href="#sgGWR.optimizers.sg.SGD" title="sgGWR.optimizers.sg.SGD"><code class="xref py py-class docutils literal notranslate"><span class="pre">SGD</span></code></a></p>
<p>Stochastic Variance Reduced Gradient with mini-batch</p>
<p>refernces:
Johnson, R., &amp; Zhang, T. (2013).
Accelerating Stochastic Gradient Descent using Predictive Variance Reduction.
In Advances in Neural Information Processing Systems (Vol. 26).</p>
<p>Allen-Zhu, Z. (2018).
Katyusha X: Practical Momentum Method for Stochastic Sum-of-Nonconvex Optimization.
35th International Conference on Machine Learning, ICML 2018, 1, 284–290.</p>
<dl class="py method">
<dt class="sig sig-object py" id="sgGWR.optimizers.SVRG.batch_step">
<span class="sig-name descname"><span class="pre">batch_step</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">f_step</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">x0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">idxs</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#sgGWR.optimizers.SVRG.batch_step" title="Permalink to this definition"></a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="sgGWR.optimizers.SVRG.run">
<span class="sig-name descname"><span class="pre">run</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">model</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">max_epoch</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">100</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">batchsize</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">PRNGkey</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">Array([0,</span> <span class="pre">123],</span> <span class="pre">dtype=uint32)</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">diff_mode</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'manual'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">tol</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.001</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">n_iter_no_change</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">5</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">min_epoch</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">5</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">verbose</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">lax_scan</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#sgGWR.optimizers.SVRG.run" title="Permalink to this definition"></a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="sgGWR.optimizers.SVRG.step">
<span class="sig-name descname"><span class="pre">step</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">t</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">x</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">f</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">g</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">f_and_g</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">idx</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#sgGWR.optimizers.SVRG.step" title="Permalink to this definition"></a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="sgGWR.optimizers.Yogi">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">sgGWR.optimizers.</span></span><span class="sig-name descname"><span class="pre">Yogi</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">learning_rate0</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1.0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">b1</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.9</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">b2</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.999</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">eps</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1e-08</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">correct_bias</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#sgGWR.optimizers.Yogi" title="Permalink to this definition"></a></dt>
<dd><p>Bases: <a class="reference internal" href="#sgGWR.optimizers.sg.Adam" title="sgGWR.optimizers.sg.Adam"><code class="xref py py-class docutils literal notranslate"><span class="pre">Adam</span></code></a></p>
<dl class="py method">
<dt class="sig sig-object py" id="sgGWR.optimizers.Yogi.step">
<span class="sig-name descname"><span class="pre">step</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">t</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">x</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">f</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">g</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">f_and_g</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">idx</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#sgGWR.optimizers.Yogi.step" title="Permalink to this definition"></a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="sgGWR.optimizers.optax_optimizer">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">sgGWR.optimizers.</span></span><span class="sig-name descname"><span class="pre">optax_optimizer</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">optax_optim=GradientTransformation(init=&lt;function</span> <span class="pre">chain.&lt;locals&gt;.init_fn&gt;</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">update=&lt;function</span> <span class="pre">chain.&lt;locals&gt;.update_fn&gt;)</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#sgGWR.optimizers.optax_optimizer" title="Permalink to this definition"></a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">object</span></code></p>
<dl class="py method">
<dt class="sig sig-object py" id="sgGWR.optimizers.optax_optimizer.run">
<span class="sig-name descname"><span class="pre">run</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">model</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">maxiter</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1000</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">batchsize</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">100</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">PRNGkey</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">diff_mode</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'manual'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">tol</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.001</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">n_iter_no_change</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">100</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">verbose</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#sgGWR.optimizers.optax_optimizer.run" title="Permalink to this definition"></a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="sgGWR.optimizers.scipy_L_BFGS_B">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">sgGWR.optimizers.</span></span><span class="sig-name descname"><span class="pre">scipy_L_BFGS_B</span></span><a class="headerlink" href="#sgGWR.optimizers.scipy_L_BFGS_B" title="Permalink to this definition"></a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">object</span></code></p>
<p>same setting to the ‘scgwr’ package in R
see:  <a class="reference external" href="https://github.com/cran/scgwr/blob/master/R/scgwr.R">https://github.com/cran/scgwr/blob/master/R/scgwr.R</a></p>
<dl class="py method">
<dt class="sig sig-object py" id="sgGWR.optimizers.scipy_L_BFGS_B.run">
<span class="sig-name descname"><span class="pre">run</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">model</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">diff_mode</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'auto'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">tol</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">kwargs_minimize</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">{}</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#sgGWR.optimizers.scipy_L_BFGS_B.run" title="Permalink to this definition"></a></dt>
<dd></dd></dl>

</dd></dl>

</section>
</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="sgGWR.html" class="btn btn-neutral float-left" title="sgGWR package" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2023, Hayato Nishi.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>